# llm_answer_context.py – Generate answers using OpenAI LLM with top match context
"""
Functions
---------
get_llm_answer(query, df_metadata, index, memory=None)
    → Returns a natural language answer generated by LLM using context from vector DB + prior memory.
    → Also returns the top 5 match records (with score) used for evaluation/logging.
"""
import streamlit as st
import os
import openai
from embedding_utils import embed
import numpy as np
from dotenv import load_dotenv

# Load your OpenAI API key from environment variable
load_dotenv()

#openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Use gpt-4o-mini or gpt-4-turbo depending on cost/speed/accuracy tradeoff
model = "gpt-4o-mini"


# ───────────────────────────── Build Prompt ─────────────────────────────
def build_prompt(query, context_chunks, memory=None):
    """
    Compose a prompt for OpenAI LLM using past memory + top match metadata-rich context.
    """
    context_block = "\n\n".join(
        f"[{i+1}] Schema: {row.get('schema', '')} | "
        f"Database: {row.get('database', '')} | "
        f"Database Description: {row.get('database_description', '')} | "
        f"Table: {row.get('table', '')} | "
        f"Table Description: {row.get('table_comment', '')} | "
        f"Column: {row.get('column', '')} | "
        f"Column Description: {row.get('column_comment', '')} | "
        f"Column Type: {row.get('data_type', '')} ({row.get('column_type', '')}) | "
        f"Column Nullable: {row.get('nullable', '')}, "
        f"Column Key: {row.get('key', '')}, "
        f"Column Length: {row.get('length', 'N/A')} | "
        f"Similarity score: {row.get('distance', 0.0):.4f}"
        for i, row in enumerate(context_chunks)
    )

    history_block = ""
    if memory:
        for turn in memory:
            history_block += f"User: {turn['query']}\nMarwin: {turn['answer']}\n"

    prompt = f"""
            You are Marwin, a helpful assistant who helps data analysts find metadata.
            
            For your reference, here is a sample schema strcuture that you would find in the metadata vector embeddings:
            Schema: cwc.fact_application.app_id
            Explanation of the above schema: cwc is the name of the database, fact_application is the table within the cwc database and app_id is the column in the table.
            
            Answer the user in plain English, telling them which database/table/column holds the data and why it matches. Provide them with appropriate description of schema, table and columns wherever               necessary. Provide a sample SQL if useful. If user needs an explaination or details about SQL quyery or understanding of SQL, please help accordingly.

            If for a given schema, database, table or a coulun, there is no desctiption in the embeddings, please DO NOT assume, DO NOT search online for context and DO NOT proceed to give details.                   Please inform the user that the description  is missing and it is suggested that they raise a Data Issue or reach out the Chief Data Office via Teams or email to cdo@example.com

            If a schema, database, table or a column that the user is enquiring about DOES NOT exist in the metadata embeddings, please DO NOT assume, DO NOT search online for context and DO NOT proceed              to give details. Please inform the user that the schema, database, table or a column you are looking for  is missing and it is suggested that they raise a Data Issue or reach out the Chief                Data Office via Teams or email to cdo@example.com
            
            Please additionally guide the user to provide appropriate prompt if the prompt is not related to user asking questions on data or metadata or SQL queries you suggested. 
            E.g., this is not a valid question - Who is the President of India?
            
            If the user thanks you or appreciates you for the help, please acknowledge and you might as well say "You are Welcome."
            
            If you are asked "Who are you or what are you, please respond that you are Marwin, a metadata assistant helping user improve Analytics productivity. Please add approppriate context if needed              and also provide an example prompt that use can refer to.

            If a user asks about databases, tables or columns OR if they come to you searching for databases, tables or columns, please use the folliwng details from the embeddings schema name or                     database name, schema description or database description, table name, table description, column name, column description.
            Example: A user asks "How many databases are there? you should look up the above embeddings and answer something like "There are X number of databases and their names are XX                               and here are their descriptions or here is what they store"
          
            
            Below is the context you can refer to:

            {context_block}

            {history_block}
            User: {query}
            Marwin:"""
    
    return prompt.strip()

#---------------------------------------------------------

def get_llm_answer(query, df_metadata, index, memory):
    """
    Embeds query, retrieves top matches, builds prompt with memory, and sends to LLM.
    Returns:
        answer (str): Natural language answer
        top_matches (List[Dict]): top-5 context matches (with distance scores and metadata)
    """
    # Embed user query
    q_vec = embed([query])[0]

    # Vector search for top matches
    D, I = index.search(np.array([q_vec]), k=5)
    top_matches = []
    for dist, idx in zip(D[0], I[0]):
        row = df_metadata.iloc[idx]
        
        top_matches.append({
            "distance": float(dist),
            "doc": row["doc"],
            
            "schema": row.get("FULL_SCHEMA", ""),
            "database": row.get("TABLE_SCHEMA", ""),
            "database_description": row.get("DATABASE_DESCRIPTION", ""),
            "table": row.get("TABLE_NAME", ""),
            "table_comment": row.get("TABLE_COMMENT", ""),
            "column": row.get("COLUMN_NAME", ""),
            "column_comment": row.get("COLUMN_COMMENT", ""),
            "data_type": row.get("DATA_TYPE", ""),
            "column_type": row.get("COLUMN_TYPE", ""),
            "nullable": row.get("IS_NULLABLE", ""),
            "key": row.get("COLUMN_KEY", ""),
            "length": row.get("CHARACTER_MAXIMUM_LENGTH", "N/A") or "N/A",
        })

    # Prompt with matches + memory
    prompt = build_prompt(query, top_matches, memory=memory)

    # Query OpenAI
    response = openai.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ]
    )

    return response.choices[0].message.content.strip(), top_matches



#-----------------------------------------------------------------------------------------------------


def get_llm_answer_old(query, df_metadata, index, memory=None, k=5):
    """
    Generates an LLM response for a user's metadata question.

    Args:
        query (str): The user question
        df_metadata (DataFrame): Metadata source
        index (FAISS index): Search index
        memory (list): Optional list of past Q&A turns
        k (int): Number of metadata matches to include

    Returns:
        str: Natural language answer from OpenAI
    """
    # Step 1: Embed user query and search
    q_vec = embed([query])[0]
    D, I = index.search(np.array([q_vec]), k)

    # Step 2: Get top-matching metadata docs
    context_chunks = [df_metadata['doc'].iloc[idx] for idx in I[0]]

    # Step 3: Format recent memory
    memory_text = "\n".join([f"You: {turn['query']}\nMarwin: {turn['answer']}" for turn in memory]) if memory else ""

    # Step 4: Compose full prompt
    prompt = f"""
    You are Marwin, a helpful assistant who helps data analysts find metadata.
    Here is the past conversation:
    {memory_text}
    
    Here is the metadata context:
    {chr(10).join(context_chunks)}
    
    Answer the user in plain English, telling them which database/table/column holds the data and why it matches.  Provide a sample SQL if useful. 
    Please additionally guide the user to provide appropriate prompt if the prompt is not related to user asking questions on data or metadata. 
    E.g., this is not a valid question - Who is the President of India? 
    {query}
    """

    response = openai.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are Marwin, a metadata assistant."},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message.content.strip()

    #------------------------------------------------------
