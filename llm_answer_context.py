# llm_answer_context.py – Generate answers using OpenAI LLM with top match context
"""
Functions
---------
get_llm_answer(query, df_metadata, index, memory=None)
    → Returns a natural language answer generated by LLM using context from vector DB + prior memory.
    → Also returns the top 5 match records (with score) used for evaluation/logging.
"""

import os
import openai
from embedding_utils import embed
import numpy as np
from dotenv import load_dotenv

# Load your OpenAI API key from environment variable
load_dotenv()

openai.api_key = os.getenv("OPENAI_API_KEY")

# Use gpt-4o-mini or gpt-4-turbo depending on cost/speed/accuracy tradeoff
model = "gpt-4o-mini"


# ───────────────────────────── Build Prompt ─────────────────────────────
def build_prompt(query, context_chunks, memory=None):
    """
    Compose a prompt for OpenAI LLM using past memory + top match metadata-rich context.
    """
    context_block = "\n\n".join(
        f"[{i+1}] Database: {row['database']}, Table: {row['table']} ({row['table_comment']}), "
        f"Column: {row['column']} ({row['column_comment']}), Type: {row['data_type']} "
        f"[{row['column_type']}], Nullable: {row['nullable']}, Key: {row['key']}, "
        f"Length: {row['length']}, Similarity score: {row['distance']:.4f}"
        for i, row in enumerate(context_chunks)
    )

    history_block = ""
    if memory:
        for turn in memory:
            history_block += f"User: {turn['query']}\nMarwin: {turn['answer']}\n"

    prompt = f"""
            You are Marwin, a helpful assistant who helps data analysts find metadata.
            Answer the user in plain English, telling them which database/table/column holds the data and why it matches. Provide a sample SQL if useful. If user needs an   explaination or details about SQL quyery or understanding of SQL, please help accordingly.
            Please additionally guide the user to provide appropriate prompt if the prompt is not related to user asking questions on data or metadata or SQL queries you suggested. 
            E.g., this is not a valid question - Who is the President of India?
            If the user thanks you or appreciates you for the help, please acknowledge and you might as well say "You are Welcome."
            
            Below is the context you can refer to:

            {context_block}

            {history_block}
            User: {query}
            Marwin:"""
    
    return prompt.strip()

#---------------------------------------------------------

def get_llm_answer(query, df_metadata, index, memory):
    """
    Embeds query, retrieves top matches, builds prompt with memory, and sends to LLM.
    Returns:
        answer (str): Natural language answer
        top_matches (List[Dict]): top-5 context matches (with distance scores and metadata)
    """
    # Embed user query
    q_vec = embed([query])[0]

    # Vector search for top matches
    D, I = index.search(np.array([q_vec]), k=5)
    top_matches = []
    for dist, idx in zip(D[0], I[0]):
        top_matches.append({
            "distance": float(dist),
            "doc": df_metadata["doc"].iloc[idx],
            "database": df_metadata["TABLE_SCHEMA"].iloc[idx],
            "table": df_metadata["TABLE_NAME"].iloc[idx],
            "table_comment": df_metadata["TABLE_COMMENT"].iloc[idx],
            "column": df_metadata["COLUMN_NAME"].iloc[idx],
            "column_comment": df_metadata["COLUMN_COMMENT"].iloc[idx],
            "data_type": df_metadata["DATA_TYPE"].iloc[idx],
            "column_type": df_metadata["COLUMN_TYPE"].iloc[idx],
            "nullable": df_metadata["IS_NULLABLE"].iloc[idx],
            "key": df_metadata["COLUMN_KEY"].iloc[idx],
            "length": df_metadata["CHARACTER_MAXIMUM_LENGTH"].iloc[idx],
        })

    # Prompt with matches + memory
    prompt = build_prompt(query, top_matches, memory=memory)

    # Query OpenAI
    response = openai.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ]
    )

    return response.choices[0].message.content.strip(), top_matches



#-----------------------------------------------------------------------------------------------------


def get_llm_answer_old(query, df_metadata, index, memory=None, k=5):
    """
    Generates an LLM response for a user's metadata question.

    Args:
        query (str): The user question
        df_metadata (DataFrame): Metadata source
        index (FAISS index): Search index
        memory (list): Optional list of past Q&A turns
        k (int): Number of metadata matches to include

    Returns:
        str: Natural language answer from OpenAI
    """
    # Step 1: Embed user query and search
    q_vec = embed([query])[0]
    D, I = index.search(np.array([q_vec]), k)

    # Step 2: Get top-matching metadata docs
    context_chunks = [df_metadata['doc'].iloc[idx] for idx in I[0]]

    # Step 3: Format recent memory
    memory_text = "\n".join([f"You: {turn['query']}\nMarwin: {turn['answer']}" for turn in memory]) if memory else ""

    # Step 4: Compose full prompt
    prompt = f"""
    You are Marwin, a helpful assistant who helps data analysts find metadata.
    Here is the past conversation:
    {memory_text}
    
    Here is the metadata context:
    {chr(10).join(context_chunks)}
    
    Answer the user in plain English, telling them which database/table/column holds the data and why it matches.  Provide a sample SQL if useful. 
    Please additionally guide the user to provide appropriate prompt if the prompt is not related to user asking questions on data or metadata. 
    E.g., this is not a valid question - Who is the President of India? 
    {query}
    """

    response = openai.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are Marwin, a metadata assistant."},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message.content.strip()

    #------------------------------------------------------
